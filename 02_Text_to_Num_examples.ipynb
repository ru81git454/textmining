{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 - Text to Num examples.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ru81git454/textmining/blob/master/02_Text_to_Num_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_YqCQ5m_0JY7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#From Textual Information to Numerical Vector - Sample Code"
      ]
    },
    {
      "metadata": {
        "id": "xsH8LHP7TboL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Getting Started"
      ]
    },
    {
      "metadata": {
        "id": "2-fbyh3sRjYI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text =\"I went to the woods because I wished to live deliberately, to front only the essential facts of life, and see if I couldn't learn what it had to teach, and not, when I came to die, discover that I had not lived.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vylKBtfDVrDB",
        "colab_type": "code",
        "outputId": "caa8603e-4653-4d24-8aac-85fd9867dc6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I went to the woods because I wished to live deliberately, to front only the essential facts of life, and see if I couldn't learn what it had to teach, and not, when I came to die, discover that I had not lived.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gTr_YssoTuK6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Word Tokenization"
      ]
    },
    {
      "metadata": {
        "id": "0d9L0Xkfbnk4",
        "colab_type": "code",
        "outputId": "edd0a3a9-cee4-47e4-bf65-10af358d39cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# tokenization using wordpunct_tokenize function\n",
        "import nltk  #nltk stand for natural language toolkit\n",
        "from nltk import wordpunct_tokenize\n",
        "punct_token = wordpunct_tokenize(text)\n",
        "print(punct_token)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'went', 'to', 'the', 'woods', 'because', 'I', 'wished', 'to', 'live', 'deliberately', ',', 'to', 'front', 'only', 'the', 'essential', 'facts', 'of', 'life', ',', 'and', 'see', 'if', 'I', 'couldn', \"'\", 't', 'learn', 'what', 'it', 'had', 'to', 'teach', ',', 'and', 'not', ',', 'when', 'I', 'came', 'to', 'die', ',', 'discover', 'that', 'I', 'had', 'not', 'lived', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hrRsmKIybosa",
        "colab_type": "code",
        "outputId": "7e900aef-1f6a-4ebb-d9a6-e9ffd2101405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "## tokenization using word_tokenize function\n",
        "nltk.download('punkt')\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['I', 'went', 'to', 'the', 'woods', 'because', 'I', 'wished', 'to', 'live', 'deliberately', ',', 'to', 'front', 'only', 'the', 'essential', 'facts', 'of', 'life', ',', 'and', 'see', 'if', 'I', 'could', \"n't\", 'learn', 'what', 'it', 'had', 'to', 'teach', ',', 'and', 'not', ',', 'when', 'I', 'came', 'to', 'die', ',', 'discover', 'that', 'I', 'had', 'not', 'lived', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y7-EViDqUPm7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "The nltk package has several implementations for stemmers. These stemmers are\n",
        "implemented in the stem module. Here we use PorterStemmer and  LancasterStemmer"
      ]
    },
    {
      "metadata": {
        "id": "AA9n6LevUPZr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "LJtHfttZUpYE",
        "colab_type": "code",
        "outputId": "99fa1fd8-0a1c-4bd3-9912-0ae487c28eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#The Porter stemming algorithm\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print (ps.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'))\n",
        "print (ps.stem('lying'), ps.stem('strange'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jump jump jump\n",
            "lie strang\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "9a695cf7-3326-48bd-90bb-45059290b84f",
        "id": "_87hJjaeVhRL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#The Lancaster stemming algorithm\n",
        "from nltk.stem import LancasterStemmer\n",
        "ls = LancasterStemmer()\n",
        "print (ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'))\n",
        "print (ls.stem('lying'), ls.stem('strange'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jump jump jump\n",
            "lying strange\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qppglj_1rcjV",
        "colab_type": "code",
        "outputId": "595386d5-9d24-43ad-cfb6-5f9523c3751e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#stemming an tokenize word list\n",
        "token_stem = [ps.stem(w) for w in tokens]\n",
        "print(token_stem)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'went', 'to', 'the', 'wood', 'becaus', 'I', 'wish', 'to', 'live', 'deliber', ',', 'to', 'front', 'onli', 'the', 'essenti', 'fact', 'of', 'life', ',', 'and', 'see', 'if', 'I', 'could', \"n't\", 'learn', 'what', 'it', 'had', 'to', 'teach', ',', 'and', 'not', ',', 'when', 'I', 'came', 'to', 'die', ',', 'discov', 'that', 'I', 'had', 'not', 'live', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h9vu2fVYm9xo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Lemmatization"
      ]
    },
    {
      "metadata": {
        "id": "iVcw5b5Sm7Lr",
        "colab_type": "code",
        "outputId": "c53969a7-9da3-41a1-a0e9-1af052856886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "# lemmatize verb/nouns\n",
        "print (wnl.lemmatize('lying', \"v\"))\n",
        "print (wnl.lemmatize('lying', \"n\"))\n",
        "print (wnl.lemmatize('lying')) # with out POS, noun is default\n",
        "print (wnl.lemmatize('saddest', 'a'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "lie\n",
            "lying\n",
            "lying\n",
            "sad\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jt51YTnvuj6z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Remove stopwords"
      ]
    },
    {
      "metadata": {
        "id": "xBCqJ069oCi6",
        "colab_type": "code",
        "outputId": "821c9d68-f14b-446f-c1bf-90612c16e26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words('english'))\n",
        "print(stopwords)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "{'d', 'am', 'into', \"you've\", \"you'd\", 'during', \"needn't\", 'against', \"couldn't\", 'below', 'under', \"doesn't\", 'mustn', \"it's\", 'himself', 'whom', 'there', 'he', 'doing', \"isn't\", 'then', 'same', 'won', 'that', 'them', 're', 'didn', \"haven't\", \"shan't\", 'about', 'don', 'these', 'weren', 'between', 'of', 'above', 'yours', 'been', 'they', 'few', 'having', 'where', 'a', 'ma', 'when', 'the', 'what', 'shan', 'hadn', 'themselves', \"mightn't\", \"weren't\", 'hasn', 'through', 'isn', 'up', 'each', 'herself', 'to', 'from', 'has', 'his', 'she', 'their', 'this', 'did', 'her', 'no', \"should've\", \"wouldn't\", 'as', 'an', 'here', 'if', 'over', 'needn', 'my', 'are', 'ourselves', 'not', 'again', 's', 'but', 'your', 'only', 'couldn', 'being', 'we', 'be', 'most', 'than', \"shouldn't\", 'after', 'haven', \"wasn't\", 'for', 'those', 'more', \"aren't\", 'do', 'now', 'such', 't', 'had', 'ain', \"didn't\", 'myself', 'before', 'is', 'wouldn', 'and', 'own', 'wasn', 'by', 'was', 'once', 'it', 'll', 've', 'both', \"hadn't\", 'hers', 'off', 'all', 'doesn', 'ours', 'why', 'so', 'with', 'should', \"you'll\", 'aren', 'shouldn', 'our', 'any', 'yourselves', 'how', 'which', \"don't\", 'y', 'theirs', 'further', 'other', \"mustn't\", 'have', 'nor', 'because', 'you', \"she's\", \"that'll\", 'some', 'down', 'too', 'yourself', 'can', 'o', 'at', \"hasn't\", 'does', 'on', \"you're\", 'its', 'who', 'very', 'him', 'or', \"won't\", 'until', 'were', 'in', 'just', 'out', 'm', 'will', 'me', 'itself', 'mightn', 'while', 'i'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IRQ6rVsEcOwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Add punctuation into stopword set"
      ]
    },
    {
      "metadata": {
        "id": "BBhF6Vd3cGil",
        "colab_type": "code",
        "outputId": "9b063c3a-6e97-4ffc-d17e-c86f48cd049c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import string\n",
        "string.punctuation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "metadata": {
        "id": "EHnhEvKIcT2z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H3z7n-GZqYxY",
        "colab_type": "code",
        "outputId": "7fc11378-d149-4737-9cd1-20d3d7336e30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "filtered_tokens = [w for w in tokens if not w in stopwords]\n",
        "print(filtered_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'went', 'woods', 'I', 'wished', 'live', 'deliberately', ',', 'front', 'essential', 'facts', 'life', ',', 'see', 'I', 'could', \"n't\", 'learn', 'teach', ',', ',', 'I', 'came', 'die', ',', 'discover', 'I', 'lived', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KtkXmpYR2jQ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part of Speed (POS) Tagging\n"
      ]
    },
    {
      "metadata": {
        "id": "WN6WXS8LrMYi",
        "colab_type": "code",
        "outputId": "31d4d259-bcf7-4cb1-cd48-1cbf0ff75fb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "#averaged_perceptron_tagger\n",
        "token_tag = nltk.word_tokenize(\"I went to the woods because I wished to live deliberately, to front only the essential facts of life\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "pos_tokens = nltk.pos_tag(token_tag)\n",
        "print(pos_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('I', 'PRP'), ('went', 'VBD'), ('to', 'TO'), ('the', 'DT'), ('woods', 'NNS'), ('because', 'IN'), ('I', 'PRP'), ('wished', 'VBD'), ('to', 'TO'), ('live', 'VB'), ('deliberately', 'RB'), (',', ','), ('to', 'TO'), ('front', 'VB'), ('only', 'RB'), ('the', 'DT'), ('essential', 'JJ'), ('facts', 'NNS'), ('of', 'IN'), ('life', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NN0fi3VmDjoB",
        "colab_type": "code",
        "outputId": "dea8b828-d961-408f-9317-dd12d281f322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('universal_tagset')\n",
        "pos_tokens = nltk.pos_tag(token_tag,tagset='universal')\n",
        "print(pos_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[('I', 'PRON'), ('went', 'VERB'), ('to', 'PRT'), ('the', 'DET'), ('woods', 'NOUN'), ('because', 'ADP'), ('I', 'PRON'), ('wished', 'VERB'), ('to', 'PRT'), ('live', 'VERB'), ('deliberately', 'ADV'), (',', '.'), ('to', 'PRT'), ('front', 'VERB'), ('only', 'ADV'), ('the', 'DET'), ('essential', 'ADJ'), ('facts', 'NOUN'), ('of', 'ADP'), ('life', 'NOUN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rumDew-zLk-L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Normalization"
      ]
    },
    {
      "metadata": {
        "id": "RwPoadlxLgn6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence=\"I went to the woods because I wished to live deliberately, to front only the essential facts of life...\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vTnJ7BeGQbMe",
        "colab_type": "code",
        "outputId": "005087a4-3ea7-4efb-fbe2-d659b9273b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sentence.lower()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i went to the woods because i wished to live deliberately, to front only the essential facts of life'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "dw9ihPfIQ-xB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence.upper()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kHUHymO_RAGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence.capitalize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hLgK1klHQtTs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TEFmsFYtRVh_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mLacA_jIRbeL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence[1:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmJOSLOGQW2q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"A\".isupper()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQBozm3oQbM7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'woods'.isalpha()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lt-uH9kWQbM_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'20'.isdigit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0o-NF5a6QbNH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'20'.isdecimal()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BdsLmBN3QbNK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'12ab'.isalnum() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dSDBrY89Vjh7",
        "colab_type": "code",
        "outputId": "262cfc20-59bc-4726-c661-02320bd03eb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "sentence=\"I went to the woods because I wished to live deliberately, to front only the essential facts of life...\"\n",
        "text_removed_pun=re.sub(r'[^\\w]',' ',sentence) \n",
        "print(text_removed_pun)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I went to the woods because I wished to live deliberately  to front only the essential facts of life   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_XQddvY_XWt0",
        "colab_type": "code",
        "outputId": "42f40d3a-81aa-4f83-b8cf-04c07c5b9076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#tokenize using split()\n",
        "tokens_split = text_removed_pun.lower().split()\n",
        "print (tokens_split )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'went', 'to', 'the', 'woods', 'because', 'i', 'wished', 'to', 'live', 'deliberately', 'to', 'front', 'only', 'the', 'essential', 'facts', 'of', 'life']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Ea7rngmpsfp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Term Frequency"
      ]
    },
    {
      "metadata": {
        "id": "VO4aDFkONBDq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V80Kmw0SprK_",
        "colab_type": "code",
        "outputId": "beab6175-3da4-4db2-d934-0b32d154c1e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "counter_text = Counter(tokens_split)\n",
        "print(counter_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'to': 3, 'i': 2, 'the': 2, 'went': 1, 'woods': 1, 'because': 1, 'wished': 1, 'live': 1, 'deliberately': 1, 'front': 1, 'only': 1, 'essential': 1, 'facts': 1, 'of': 1, 'life': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qaIyd4uNuAYz",
        "colab_type": "code",
        "outputId": "4fa9f51b-602b-41cc-c26a-244487b88a92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# calculate the word frequency\n",
        "fdist = nltk.FreqDist(tokens_split)\n",
        "fdist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'because': 1,\n",
              "          'deliberately': 1,\n",
              "          'essential': 1,\n",
              "          'facts': 1,\n",
              "          'front': 1,\n",
              "          'i': 2,\n",
              "          'life': 1,\n",
              "          'live': 1,\n",
              "          'of': 1,\n",
              "          'only': 1,\n",
              "          'the': 2,\n",
              "          'to': 3,\n",
              "          'went': 1,\n",
              "          'wished': 1,\n",
              "          'woods': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "cLVgLJIUuvaj",
        "colab_type": "code",
        "outputId": "6f20aec2-5d37-4f12-9e6d-2017b5886114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "# print out the most 10 common word in filtered_hamlet\n",
        "print (tabulate(fdist.most_common(10), headers=[\"word\",\"times\"]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word            times\n",
            "------------  -------\n",
            "to                  3\n",
            "i                   2\n",
            "the                 2\n",
            "went                1\n",
            "woods               1\n",
            "because             1\n",
            "wished              1\n",
            "live                1\n",
            "deliberately        1\n",
            "front               1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nYBcP0Udqyes",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Tf-idf"
      ]
    },
    {
      "metadata": {
        "id": "9woXVOnJq2fg",
        "colab_type": "code",
        "outputId": "be7c561a-5d67-4ad3-acad-0dec512de171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "doc1=\"this is a good sample\"\n",
        "doc2= \"this is another example another example example\"\n",
        "doc1_tokens= doc1.split()\n",
        "doc2_tokens=doc2.split()\n",
        "print(\"doc1_token:\",doc1_tokens)\n",
        "print(\"doc2_token:\",doc2_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doc1_token: ['this', 'is', 'a', 'good', 'sample']\n",
            "doc2_token: ['this', 'is', 'another', 'example', 'another', 'example', 'example']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xRJTKCkUveLz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create tf function\n",
        "def tf(term, token_doc):\n",
        "    tf = token_doc.count(term)/len(token_doc)\n",
        "    return tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7SX0yM5bv8gm",
        "colab_type": "code",
        "outputId": "33748840-0f54-4ced-8db3-6b350ffcc50c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"tf(this,doc1)=\",tf(\"this\",doc1_tokens))\n",
        "print(\"tf(example,doc2)=\",tf(\"example\",doc2_tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf(this,doc1)= 0.2\n",
            "tf(example,doc2)= 0.42857142857142855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9TZuDqsbxtxO",
        "colab_type": "code",
        "outputId": "a6eb03df-1830-44bf-fa77-4f29bee90083",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "token_doclist =[doc1_tokens, doc2_tokens]\n",
        "print (\"Doc list: \", token_doclist)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Doc list:  [['this', 'is', 'a', 'good', 'sample'], ['this', 'is', 'another', 'example', 'another', 'example', 'example']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-9fqbuWUxVJD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create function to calculate how many doc contain the term \n",
        "def numDocsContaining(word, token_doclist):\n",
        "    doccount = 0\n",
        "    for doc_token in token_doclist:\n",
        "        if doc_token.count(word) > 0:\n",
        "            doccount +=1\n",
        "    return doccount "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R8P63E9Bzwyx",
        "colab_type": "code",
        "outputId": "7a6ebc40-b3df-4560-c2a9-67c865d75341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print (\"numDocsContaining 'this':\",numDocsContaining(\"this\",token_doclist))\n",
        "print (\"numDocsContaining 'example':\",numDocsContaining(\"example\",token_doclist))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numDocsContaining 'this': 2\n",
            "numDocsContaining 'example': 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RkfTCSPC1zeo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "# create function to calculate  Inverse Document Frequency in doclist\n",
        "def idf(word, token_doclist):\n",
        "    n = len(token_doclist)\n",
        "    df = numDocsContaining(word, token_doclist)\n",
        "    return math.log10(n/df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHnKFIGe2NE_",
        "colab_type": "code",
        "outputId": "7e686a02-0f56-4631-a675-d9263e24be6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print (\"idf of 'this':\",idf(\"this\",token_doclist))\n",
        "print (\"idf of 'example':\",idf(\"example\",token_doclist))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "idf of 'this': 0.0\n",
            "idf of 'example': 0.3010299956639812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E4X4MNjF3OKG",
        "colab_type": "code",
        "outputId": "b6f5d5ac-f964-4831-d452-728110d9498c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print (\"tfidf of 'this' in doc1:\", tf(\"this\",doc1_tokens)*idf(\"this\",token_doclist))\n",
        "print (\"tfidf of 'example' in doc1:\", tf(\"example\",doc1_tokens)*idf(\"example\",token_doclist))\n",
        "print (\"tfidf of 'example' in doc2:\", tf(\"example\",doc2_tokens)*idf(\"example\",token_doclist))\n",
        "                                                                    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfidf of 'this' in doc1: 0.0\n",
            "tfidf of 'example' in doc1: 0.0\n",
            "tfidf of 'example' in doc2: 0.12901285528456335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2bgwPZwGBjl2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Count vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "MXWDLY49Bi7B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc1=\"this is a good sample\"\n",
        "doc2= \"this is another example another example example\"\n",
        "doc1_tokens= doc1.split()\n",
        "doc2_tokens=doc2.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b7Nj6PymB-M9",
        "colab_type": "code",
        "outputId": "745ec3dd-6c7b-444c-f863-fc4c4bc86244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(\"Doc 1 counter:\", Counter(doc1_tokens))\n",
        "print(\"Doc 2 counter:\", Counter(doc2_tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Doc 1 counter: Counter({'this': 1, 'is': 1, 'a': 1, 'python': 1, 'sample': 1})\n",
            "Doc 2 counter: Counter({'example': 3, 'another': 2, 'this': 1, 'is': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VI33T5hSMEL-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#TFIDF vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "WeKuldSSNfJ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define function to calculate tfidf\n",
        "def tfidf(word, doc, token_doclist):\n",
        "  tfidf = tf(word,doc)* idf(word,token_doclist)\n",
        "  return tfidf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z3_QC5CaNQqk",
        "colab_type": "code",
        "outputId": "277a9d0a-9a15-4f30-acb2-1b927aeccc2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "for term in set(doc1_tokens):\n",
        "   term_tfidf= tfidf(term,doc1_tokens,token_doclist)\n",
        "   print(term,\":\", round(term_tfidf,4))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this : 0.0\n",
            "is : 0.0\n",
            "a : 0.0602\n",
            "good : 0.0602\n",
            "sample : 0.0602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ywT7RqU6P7-3",
        "colab_type": "code",
        "outputId": "8561e81b-24cd-4cba-c6cf-b000ee0896eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "for term in set(doc2_tokens):\n",
        "   term_tfidf= tfidf(term,doc2_tokens,token_doclist)\n",
        "   print(term,\":\", round(term_tfidf,4))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example : 0.129\n",
            "another : 0.086\n",
            "is : 0.0\n",
            "this : 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KUM8z0JjkO2I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J5OW-l17eyjM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Exercise"
      ]
    },
    {
      "metadata": {
        "id": "4oI3OqHWQbNX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download \"shakespeare-macbeth\" sample of texts from Project Gutenberg appears in the NLTK corpus collection\n",
        "\n",
        "Compute the word frequencies in \"shakespeare-macbeth.txt\"\n",
        "- read sentences from file \"**shakespeare**-macbeth.txt\"\n",
        "- split sentences into words (using split(), or nltk word_tokenize)\n",
        "- filter out symbols use methods:(isalpha, isdigit, isalnum)and stop words\n",
        "- normalize words and count ('Word' and 'word' are considered as the same word)\n",
        "- count the occurence of words.\n"
      ]
    },
    {
      "metadata": {
        "id": "lwDM_3REfTSz",
        "colab_type": "code",
        "outputId": "2f3183d4-9d58-45cc-f3e5-e6fa1c0ee5bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "print(nltk.corpus.gutenberg.fileids())\n",
        "macbeth_text = open('/root/nltk_data/corpora/gutenberg/shakespeare-macbeth.txt').read()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S598Neazha8t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(macbeth_text)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}